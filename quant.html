<!DOCTYPE HTML>
<html>
<head>
<title>Project Detail</title>
<link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
<!-- jQuery (necessary JavaScript plugins) -->
<script src="js/jquery.min.js"></script>
<!-- Custom Theme files -->
 <link href="css/dashboard.css" rel="stylesheet">
<link href="css/style.css" rel='stylesheet' type='text/css' />

<!-- Custom Theme files -->
<!--//theme-style-->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="keywords" content="Curriculum Vitae Responsive web template, Bootstrap Web Templates, Flat Web Templates, Andriod Compatible web template, 
Smartphone Compatible web template, free webdesigns for Nokia, Samsung, LG, SonyErricsson, Motorola web design" />
<script type="application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); } </script>
<link href='http://fonts.googleapis.com/css?family=Ubuntu:300,400,500,700' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Varela+Round' rel='stylesheet' type='text/css'>
<!-- start menu -->
  
</head>
<body>
	
<div class="details_header col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 ">
    <ul>
        <li><a href="aboutMe.html"><span class="glyphicon glyphicon-user" aria-hidden="true"></span>About Me</a></li>
        <li><a href="index.html"><span class="glyphicon glyphicon-tasks" aria-hidden="true"></span>My Academy</a></li> 
        <li><a href="javascript:void(0);" onclick="viewpdf('transcript')"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>Transcript</a></li>
        <li><a href="javascript:void(0);" onclick="viewpdf('XiaoweiChiCV')"><span class="glyphicon glyphicon-print" aria-hidden="true"></span>Print CV</a></li>
        <!-- <li><a href="contact.html"><span class="glyphicon glyphicon-envelope" aria-hidden="true"></span>Email me</a></li> -->
        <li><a class="play-icon popup-with-zoom-anim" href="blogmain.html"><span class="glyphicon glyphicon-pencil" aria-hidden="true"></span>blogs</a></li>
    </ul>
</div>

<!-- header -->
<div class="col-sm-3 col-md-2 sidebar">
		 <div class="sidebar_top">
			 <h1>Xiaowei Chi</h1> 
			 <img src="images/g4.jpg" alt=""/>
		 </div>
		<div class="details"> 
			 <h3>EMAIL</h3>
       <p><a href="">litwellChi@gmail.com</a></p>		
       <h3>Skype</h3>
			 <p><a href="">1033808973</a></p>	 
		</div>
		<div class="clearfix"></div>
</div>
<!---->
<link href="css/popuo-box.css" rel="stylesheet" type="text/css" media="all"/>
<script src="js/jquery.magnific-popup.js" type="text/javascript"></script>
	<!---//pop-up-box---->			

	
<div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
	 <div class="content" style="padding:1em 0 0 0;">
		 <div class="education">
			 <h3 class="clr3" id="ire">Project Detail page</h3>
			 <div class="education_details">
				 <h4>Investigation on Removing Batch Normalization for Efficient Full 8-bit Integer DNN Online Training<span>07/2019 – 11/2020</span></h4>
				 <h6>Part-time Research Assistant</h6>
				 <h6>Supervisor: Guoqi Li</h6>
          <h5 style="padding: 1em 0 0 1; background-color: #D1EDC5;"><b>Step One:</b></h5>
          <br><b>◼ Reproduction and Exploration of Thesis</b>
          <br>&ensp;• 8-bit Quantization
          <br><img src="images/quant/datastream.jpg"/>
          <br>&emsp; ➢ Referred to the paper's method and trained the ResNet network with ImageNet data set in the 8-bit data stream.
          <br>&emsp; ➢ Utilized direct Quantization, constant Quantization, shift Quantization and other quantitative methods to quantize the data from 32-bit to 8-bit based on WAGEUBN, and observed the impact of the data stream.
          <br>&ensp;• Deep Neural Network without Batch Normalization (BN)
          <br>&emsp; ➢ Applied Tensorflow to reproduce FixupNet and obtained the identical results as the original paper with ImageNet data set.
          <br><h5 style="padding: 1em 0 0 1; background-color: #D1EDC5;"><b>Step Two:</b></h5>
          <br><b>◼ Model Fusion based on 8-bit and without BN</b>
          <br><img src="images/quant/forward.jpg"/>
          <br><img src="images/quant/backward.png"/>
          <br>&ensp;• Migrated all the quantitative methods mentioned above to FixupNet and screened the current quantitative methods by control variable method.
          <br>&ensp;• Utilized quantization methods based on WAGEUBN to quantize the parameters in FixupNet, it is found that the accuracy of 8-bit DNN without BN has little influence.
          <br>&ensp;• Quantified the whole forward propagation and backward propagation to 8-bit respectively.
          <br>&ensp;• For the backward propagation, the scale error needed to utilize additional flag quantification method, and then tested the model to get the training method with the least impact on accuracy.
          <br><h5 style="padding: 1em 0 0 1; background-color: #D1EDC5;"><b>Step Three:</b></h5>
          <br><b>◼ Feasibility Verification and Analysis</b>
          <br>&ensp;• Analyzed the L2 normal form of gradient updating to find the network reaches block dynamic isometry (BDI) when the average trace of the Jacobian matrix of each block approaches to 1.
          <br>&ensp;• Analyzed the ResNet with BN and FixupNet without BN with mathematical theory, and they both reached BDI.
          <br>&ensp;• For the shortcomings of BN in small batch size, and the network without BN can be trained in small batch size for theoretical proof. And utilized ResNet-110 to verify the above content.
          <br><h5 style="padding: 1em 0 0 1; background-color: #D1EDC5;"><b>Step Four:</b></h5>
          <br><b>◼ Result</b>
          <br>&ensp;• Compared with ResNet-34 and ResNet-50, the accuracy of 8bit-Fixup34 and 8bit-Fixup50 models is only 1.69% and 3.46% lower than the former.
          <br>&ensp;• For ResNet-110 network models with different batch (1, 2, 4, 8,128), the accuracy of the model with BN is 22.74%, 85.05%, 88.41%, 90.36% and 92.84%, respectively, while without BN is 89.41%, 92.74%, 93.11%, 93.13% and 92.98%.
          <br><img src="images/quant/smallBatchResult.jpg"/>
          
          <br><b>◼ Conclusions</b>
          <br>&ensp;• Batch Normalization can be replaced by appropriate initialization in the large DNN such as ResNet-110 and ResNet-101 to reduce the complex operations during the forward and backward propagation of BN. And provide a possibility to reduce the memory cost (training a DNN without large batch size).
          <br>&ensp;• The 8bit neural network training in the case of small batch size is realized, and the accuracy loss is almost 0. And the memory required for training is reduced by more than ten times.
          <br>&ensp;• A quantization method for networks without BN is proposed. Compared with the previous work using BN, this method can still maintain a very high accuracy while ensuring a small loss of accuracy. Even when without BN, training in small batches can be achieved without affecting accuracy
				 <p class="cmpny1"></p>


			 </div>

		 </div>
	</div>
</div>

<script>
    // js go to pdf page
    function viewpdf(obj){
    p1='<html><head><title>pdf</title></head><body style=\"height: 100%; width: 100%; overflow: hidden; margin:0px; background-color: rgb(51, 51, 51);\"><embed style=\"position:absolute; left: 0; top: 0;\" width=\"100%\" height=\"100%\"';
    p2= 'src=\"pdf/'+obj+'.pdf\" type=\"application/pdf\"></body></html>';
    w=window.open('about:blank');
    w.document.write(p1+p2);
    w.document.close();
    // w.print(); 

    }
</script>

<!---->
</body>
</html>
